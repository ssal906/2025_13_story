{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ce678eb1",
   "metadata": {},
   "source": [
    "# Install"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "18263a24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cuda 11.1\n",
    "#!pip install -r requirements-torch-cu111.txt --extra-index-url https://download.pytorch.org/whl/lts/1.8/cu111"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "4a96d89d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "abdb1902",
   "metadata": {},
   "outputs": [],
   "source": [
    "prefix = 'final'\n",
    "url = 'https://drive.google.com/uc?id=1akrTOnfP29ycTR7dMZDEPFrprlWZvVAD'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "db1b4fb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading...\n",
      "From (original): https://drive.google.com/uc?id=1akrTOnfP29ycTR7dMZDEPFrprlWZvVAD\n",
      "From (redirected): https://drive.google.com/uc?id=1akrTOnfP29ycTR7dMZDEPFrprlWZvVAD&confirm=t&uuid=5dd12c84-f8ca-4c8b-94cf-4f2777ce4d33\n",
      "To: c:\\Users\\sbin0\\Desktop\\스토리 생성기 mk.2\\data\\json.zip\n",
      "100%|██████████| 62.0M/62.0M [00:05<00:00, 10.7MB/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'data/json.zip'"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scripts_file = f'data/json.zip'\n",
    "gdown.download(url, scripts_file, quiet=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "8aa71a8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil, zipfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "450a0a94",
   "metadata": {},
   "outputs": [],
   "source": [
    "shutil.rmtree('data/json', ignore_errors=True)  # 기존 폴더 삭제\n",
    "with zipfile.ZipFile('data/json.zip', 'r') as zip_ref:\n",
    "    zip_ref.extractall('data/json')             # 압축 해제"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "bc1255e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "df945932",
   "metadata": {},
   "outputs": [],
   "source": [
    "json_files = sorted(glob('data/json/*/*.json'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "107e0c2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dict = []\n",
    "for json_file in json_files:\n",
    "    with open(json_file, encoding='utf-8') as f:\n",
    "        story_dict = json.load(f)\n",
    "\n",
    "    story_genre = story_dict.get('genre', ['기타'])  # ⭐ 최상단 genre 가져오기\n",
    "\n",
    "    for unit in story_dict['units']:\n",
    "        unit_dict = {}\n",
    "        unit_dict['uid'] = unit['id']\n",
    "        unit_dict['storyline'] = unit['storyline']\n",
    "        unit_dict['genre'] = story_genre  # ⭐ unit에도 장르 복사 (리스트 형태 그대로)\n",
    "        unit_dict['script'] = [s['content'] for s in unit['story_scripts']]\n",
    "        data_dict.append(unit_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "e89c36ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100077"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "759dc478",
   "metadata": {},
   "source": [
    "# Data preperation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "7c1fe417",
   "metadata": {},
   "outputs": [],
   "source": [
    "#prefix = 'final'\n",
    "#url = 'https://drive.google.com/uc?id=1Bts2h-QPQ5-m7sDIXgVRfumjl-8XHOst'\n",
    "#url = 'https://drive.google.com/uc?id=1x6HuyJTQcNydJ9P-fJl2LtxnnAu9Vp8N'\n",
    "#prefix = '1cycle'\n",
    "#url = 'https://drive.google.com/uc?id=1j46elyFZtkmnmCehlntMi0eX0Tp5nnav'\n",
    "#prefix = 'helper'\n",
    "#url = 'https://drive.google.com/uc?id=1iSP_YKFs56d5cRRTEMzfedwRxrx-nXWO'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6061a428",
   "metadata": {},
   "source": [
    "## 스토리헬퍼 샘플 데이터 다운로드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "ac09f2c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#scripts_file = f'data/scripts_{prefix}.json'\n",
    "#zip_file = f'data/scripts_{prefix}.zip'\n",
    "#gdown.download(url, zip_file, quiet=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "ef689b61",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!unzip $zip_file \n",
    "#!mv -f 'final.json' $scripts_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "af38109f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import json\n",
    "\n",
    "#with open(scripts_file) as f:\n",
    "#    data_dict = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "c880d607",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'uid': '01_0017_01',\n",
       " 'storyline': 'C001은 지하철 안에서 졸다가 깨어 내리는데 두 여자아이를 발견하고 섬찟한다.',\n",
       " 'genre': ['스릴러', '공포(호러)', '드라마'],\n",
       " 'script': ['C001은 지하철 노약자석 한구석에서 졸고 있다.',\n",
       "  'C001은 졸다가 눈을 가늘게 떴을 때 어떤 엄마와 아이를 본다.',\n",
       "  'C001은 열차가 문을 열고 닫는 동안 계속 졸다가 종착역임을 알게 되고 빠져나온다.',\n",
       "  'C001은 열차 안에 두 여자아이가 잠든 채로 문이 닫히고 출발하는 것을 본다.',\n",
       "  'C001은 섬찟하지만, 잠에서 막 깨어 피곤해한다.',\n",
       "  'C001은 집으로 들어가 부엌으로 향한다.',\n",
       "  '뭐 해?',\n",
       "  '놀래라. 소리도 없이 들어오네? 귀신인 줄 알았어. 밖에 비 와?',\n",
       "  'C001은 수건으로 머리를 닦으며 말한다.',\n",
       "  '제대로 된 우산 하나 있었는데 지난번에 차에 두고 내렸네.',\n",
       "  '할 수 없네. 갈게. 피곤해 보이니까 잘 쉬고.']}"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 샘플 데이터 출력\n",
    "data_dict[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "618293c6",
   "metadata": {},
   "source": [
    "**후처리**\n",
    "1. `\\n`을 제거한다. \"부엌에서 일하게 된 마리오\\n인부들 사이에서 인기만점인 베아트리체\"  \n",
    "   ==> 필요없는 것 같음."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "d48b59ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 비정상적 white character가 없는지 확인\n",
    "for idx, data in enumerate(data_dict):\n",
    "    #data['storyline'] = data['storyline'].replace('\\n', ' ')\n",
    "    for i, context in enumerate(data['script']):\n",
    "        #if '\\n' in context:\n",
    "        if '부엌에서 일하게' in context:\n",
    "            print(idx, i, context)\n",
    "            print('\"%s%s\"'%(context[9],context[10]))\n",
    "            print(context[10] == ' ')\n",
    "        #data['script'][i] = context.replace('\\n', ' ')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f58e2ce",
   "metadata": {},
   "source": [
    "## Tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58b08482",
   "metadata": {},
   "source": [
    "### kobigbird pretrained model을 이용한 tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "cb68ef25",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained('monologg/kobigbird-bert-base')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ab61f27",
   "metadata": {},
   "source": [
    "# Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "e81a5672",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all suitable sessions:  100077\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "EOS_ID = tokenizer.sep_token_id\n",
    "\n",
    "positive_sessions = []\n",
    "positive_str = []\n",
    "positive_ids = []\n",
    "for i, unit_data in enumerate(data_dict):\n",
    "    # 스크립트 토큰화 및 [SEP] 추가\n",
    "    unit_contexts = [tokenizer.tokenize(text) for text in unit_data['script'] + ['[SEP]']]\n",
    "    while [] in unit_contexts:\n",
    "        print('empty string in the script. removing..., id=', i)\n",
    "        index = unit_contexts.index([])\n",
    "        del unit_contexts[index]\n",
    "        del unit_data['script'][index]\n",
    "    if len(unit_contexts) <= 1:\n",
    "        print('empty scripts. skipping..., id=', i)\n",
    "        continue\n",
    "    # 내러티브 토큰화\n",
    "    unit_narrative = tokenizer.tokenize(unit_data['storyline'])\n",
    "    if len(unit_narrative) == 0:\n",
    "        print('empty narrative. skipping, id=', i)\n",
    "        continue\n",
    "\n",
    "    # 장르 정보가 반드시 포함되어야 함 (positive_sessions에만 추가됨)\n",
    "    if 'genre' not in unit_data or not unit_data['genre']:\n",
    "        print('no genre info. skipping..., id=', i)\n",
    "        continue\n",
    "    # 만약 단일 값이면 리스트로 만들고 여러 값이면 그대로 사용\n",
    "    genre_info = unit_data['genre'] if isinstance(unit_data['genre'], list) else [unit_data['genre']]\n",
    "\n",
    "    # positive_sessions 항목에 장르 정보 추가 (항목 개수를 늘리지 않음)\n",
    "    positive_sessions.append([unit_contexts, unit_narrative, 1, genre_info])\n",
    "    positive_str.append(unit_data)\n",
    "    positive_ids.append(unit_data['uid'])\n",
    "\n",
    "print(\"all suitable sessions: \", len(positive_sessions))\n",
    "\n",
    "# reproducibility를 위한 random seed 설정 및 데이터 셔플 (각 리스트 일관되게 셔플)\n",
    "np.random.seed(42)\n",
    "np.random.shuffle(positive_sessions)\n",
    "np.random.seed(42)\n",
    "np.random.shuffle(positive_str)\n",
    "np.random.seed(42)\n",
    "np.random.shuffle(positive_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "a92f433a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[['C', '##00', '##2', '##는', '강력', '##팀', '##과', 'C', '##00', '##5', '##에', '##게', '무', '##전', '##한다', '.'], ['CCTV', '상', '나이트', '##클럽', '뒷', '##골목', '##에', '들어갔', '##다가', '차', '##를', '훼손', '##시', '##켜', '##서', '렉', '##카', '##로', '끌', '##고', '나왔', '##습', '##니다', '.'], ['C', '##00', '##8', ',', 'C', '##00', '##9', ',', 'C', '##0', '##16', '##는', '무', '##전', '##을', '듣', '##고', '이동', '##한다', '.'], ['한편', ',', 'C', '##0', '##25', '##와', 'C', '##0', '##12', '부하', '##2', '##는', 'C', '##00', '##1', '##과', 'C', '##0', '##21', '##를', '죽일', '준비', '##를', '한다', '.'], ['형', '##님', ',', '우리', '어떡', '##하', '##죠', '?', '살려', '##주', '##세요', '.', '이러', '##지', '말', '##고', '대화', '##로', '해결', '##해', '##요', '.'], ['C', '##00', '##1', '##은', 'C', '##0', '##12', '부하', '##2', '##를', '부르', '##며', '조롱', '##을', '한다', '.'], ['아까', '네', '##가', '때리', '##는', '게', '너무', '간지', '##러워서', '웃', '##겼', '##어', '.'], ['뭐', '##라고', '?', '이게', '덜', '맞', '##았', '##나', '?'], ['C', '##0', '##25', '##는', 'C', '##0', '##12', '부하', '##2', '##에', '##게', '도발', '##에', '넘어가', '##지', '말라', '##고', '말린', '##다', '.'], ['C', '##0', '##25', '##는', '전기', '##톱', '##을', '들', '##고', '밖', '##으로', '나간다', '.'], ['[SEP]']], ['C', '##00', '##8', ',', 'C', '##00', '##9', ',', 'C', '##0', '##16', '##는', 'C', '##00', '##1', '##을', '구하', '##러', '출동', '##하고', 'C', '##00', '##1', '##은', 'C', '##0', '##12', '부하', '##2', '##를', '도발', '##한다', '.'], 1, ['스릴러']], [[['C', '##00', '##1', '##은', 'C', '##00', '##3', '##과', '카페', '##에', '간다', '.'], ['C', '##00', '##1', '##이', '오', '##열', '##한다', '.'], ['네', '##가', '내', '인생', '망', '##쳤', '##어', '.'], ['우리', '오빠', '##한', '##테', '##는', '뭐', '##라고', '말', '##할', '거', '##야', '?'], ['내', '##가', '너희', '집', '##에', '가', '##서', '다', '이야기', '##할', '##게', '.'], ['돌이킬', '수', '없', '##는', '짓', '##을', '저질렀', '##다고', '할', '거', '##야', '?'], ['카페', '손님', '##들', '##은', 'C', '##00', '##1', '##이', '임신', '##한', '걸로', '오해', '##한다', '.'], ['C', '##00', '##3', '##은', 'C', '##00', '##1', '##을', '달', '##랜', '##다', '.'], ['내', '##가', '다', '해결', '##할', '##게', '.'], ['내', '옆', '##에서', '계속', '지켜', '##줄', '거', '##예', '##요', '?'], ['회사', '##에', '##는', '나가', '##야', '해', '.'], ['그럴', '줄', '알', '##았', '##어', '!', '날', '버리', '##겠', '##단', '소리', '##잖아', '!'], ['C', '##00', '##1', '##은', '집', '##에', '가', '##겠다', '##고', '한다', '.'], ['C', '##00', '##3', '##은', 'C', '##00', '##1', '##을', '붙잡', '##는', '##다', '.'], ['내', '##가', '네', '인생', '목숨', '걸', '##고', '책임질', '##게', '.'], ['도망치', '##지', '마', '.', '나', '믿', '##어', '##줘', '.'], ['C', '##00', '##1', '##은', 'C', '##00', '##3', '##의', '말', '##에', '대성', '##통', '##곡', '##한다', '.'], ['카페', '손님', '##들', '##은', '둘', '##을', '보', '##며', '박수', '##를', '보낸다', '.'], ['다음', '##날', 'C', '##00', '##1', '##은', '일어나', '방', '밖', '##으로', '나간다', '.'], ['C', '##00', '##6', '##은', 'C', '##00', '##1', '##에', '##게', '아침', '##식', '##사', '##를', '챙겨', '##준다', '.'], ['이런', '거', '필요', '없', '##어요', '.', '불편', '##하', '##기', '##만', '해요', '.'], ['나', '##한', '##테', '왜', '이래', '##요', '?', '지켜', '##주', '##기', '##라도', '할', '거', '##예', '##요', '?'], ['아니요', '.', 'C', '##00', '##1', '씨', '##가', '벼랑', '끝', '##에', '몰렸', '##을', '때', '찾아와', '##요', '.'], ['구해', '##주', '##기', '##라도', '하', '##게요', '?'], ['대신', '떨어져', '줄게', '##요', '.', '나', '믿', '##어요', '.'], ['C', '##00', '##1', '##은', 'C', '##00', '##6', '##의', '말', '##에', '혼란', '##스', '##러워', '##한다', '.'], ['나', '그렇게', '나쁜', '사람', '아니', '##에', '##요', '.'], ['C', '##00', '##1', '##은', 'C', '##00', '##6', '##에', '대해', '알고', '싶', '##어', '한다', '.'], ['C', '##00', '##1', '##은', 'C', '##00', '##3', '##에', '##게', '연락', '##하', '##기', '위해', '휴대폰', '##을', '꺼낸', '##다', '.'], ['C', '##00', '##1', '##은', 'SNS', '##에서', '어젯밤', '둘', '##의', '동영상', '##이', '올라온', '것', '##을', '알', '##게', '된다', '.'], ['[SEP]']], ['C', '##00', '##3', '##은', 'C', '##00', '##1', '##을', '책임지', '##겠다', '##고', '약속', '##하고', ',', 'C', '##00', '##6', '##은', 'C', '##00', '##1', '##을', '도와', '##주', '##겠다', '##고', '한다', '.'], 1, ['멜로/로맨스', '드라마']], [[['C', '##00', '##1', '##와', 'C', '##00', '##3', '##는', '식당', '##을', '방문', '##한다', '.'], ['이때', '공', '##을', '따라', '##온', 'C', '##00', '##7', '##와', 'C', '##00', '##4', '##를', '만난다', '.'], ['C', '##00', '##1', '##는', '반갑', '##게', 'C', '##00', '##4', '##에', '##게', '인사', '##를', '한다', '.'], ['C', '##00', '##4', '##는', '그', '##들', '##에', '##게', 'C', '##00', '##2', '##가', '사회주의', '시위', '##에서', '죽', '##었', '##다고', '전한다', '.'], ['아이', '##도', '보', '##지', '못했', '##다고', '말', '##한다', '.'], ['그리고', '그', '##가', '녹음', '##한', '모든', '소리', '##들', '##을', '들려준다', '.'], ['C', '##00', '##2', '##는', 'C', '##00', '##1', '##에', '##게', '음성', '##편', '##지', '##를', '녹음', '##한다', '.'], ['C', '##00', '##1', '##가', '떠난', '뒤', '아름다움', '##을', '다시', '느끼', '##지', '못했', '##지만', ',', '녹음', '##을', '하', '##면', '##서', '그', '##가', '자기', '##를', '위해', '모든', '아름다움', '##을', '놓', '##고', '갔', '##음', '##을', '깨달', '##았', '##다고', 'C', '##00', '##2', '##의', '말', '##을', '전한다', '.'], ['시도', '썼', '##다고', '말', '##하', '##며', 'C', '##00', '##1', '##에', '##게', '고마움', '##을', '표시', '##한다', '.'], ['C', '##00', '##1', '##는', '해변', '##을', '걷', '##는', '##다', '.'], ['[SEP]']], ['C', '##00', '##1', '##는', '오랜', '시간', '##이', '지난', '후', '섬', '##을', '다시', '방문', '##한다', '.', '그리고', 'C', '##00', '##4', '##로', '##부터', 'C', '##00', '##2', '##가', '죽', '##었', '##다는', '소식', '##을', '듣', '##는', '##다', '.', '그리고', 'C', '##00', '##1', '##는', '그', '##가', '자신', '##을', '위해', '녹음', '##한', '섬', '소리', '##를', '듣', '##는', '##다', '.'], 1, ['드라마', '코미디', '멜로/로맨스']], [[['C', '##00', '##2', '##은', '술', '##이', '덜', '깨', '졸', '##고', '있', '##다', '.'], ['나타', '##샤', '##에', '대해', '브리핑', '##한다', '.'], ['문제', '##는', '나타', '##샤', '##가', '아니', '##었', '##습', '##니다', '.'], ['C', '##00', '##1', '##가', '놀이', '##공원', '##에', '있', '##는', '화면', '##이', '나온다', '.'], ['이', '여인', '##을', '주목', '##해', '##주', '##십시오', '.'], ['그냥', '임산부', '##잖아', '.'], ['C', '##00', '##1', '##가', '복', '##대', '##를', '풀', '##고', '몸', '##이', '날씬', '##해졌', '##다', '.'], ['몸', '##놀림', '##을', '볼', '때', 'C', '##00', '##7', '##의', '조직원', '##이', '##라는', '분석', '##입니다', '.'], ['C', '##00', '##1', '##의', '얼굴', '##이', '확대', '##된', '##다', '.'], ['C', '##00', '##2', '##은', 'C', '##00', '##1', '##의', '얼굴', '##을', '확인', '##하고', '놀란', '##다', '.'], ['[SEP]']], ['C', '##00', '##1', '##가', '적', '##으로', '의심', '##받', '##는', '##다', '.'], 1, ['멜로/로맨스', '액션']], [[['C', '##00', '##5', '##이', 'C', '##0', '##12', '##을', '온몸', '##으로', '밀어내', '##고', '있', '##다', '.'], ['이', '더러운', '새끼', '!', '경찰', '부른다', '!'], ['C', '##00', '##1', '##가', 'C', '##00', '##8', '##가', '데려', '##온', '배우', '##들', '면접', '##을', '본다', '.'], ['C', '##00', '##1', '##가', '배우', '##들', '##의', '육체', '##적', '##인', '부분', '##을', '체크', '##하', '##곤', '대본', '리딩', '##을', '해', '##보', '##자', '##고', '한다', '.'], ['감독', '##님', '.', '누가', '찾아왔', '##는데', '##요', '?', '병원', '##에서', '오', '##신', '거', '같', '##아', '##요', '.'], ['간호사', '##복', '##에', '뿔', '##테', '안경', '##을', '쓴', 'C', '##00', '##2', '##가', 'C', '##00', '##1', '##를', '찾아온다', '.'], ['오늘', '감독', '##님', '오디션', '보', '##는', '날', '##이', '##라면', '##서', '##요', '?', '저', '##도', '정식', '##으로', '오디션', '보', '##려고', '##요', '.', '친분', '이용', '##하', '##지', '않', '##고', '실력', '##으로', '승부', '##할', '##게요', '!'], ['C', '##00', '##1', '##가', '갑자기', '들리', '##는', '신음', '소리', '##에', '당황', '##한다', '.'], ['C', '##00', '##1', '##가', '잠시', '##만', '기다려', '달라', '##며', '후', '##다닥', '나간다', '.'], ['C', '##00', '##2', '##가', '작', '##은', '역할', '##인', '##데', '##도', '열심히', '한다며', '감탄', '##한다', '.'], ['[SEP]']], ['C', '##00', '##2', '##가', '오디션', '##을', '보', '##겠다', '##며', 'C', '##00', '##1', '##의', '사무실', '##을', '찾아온다', '.'], 1, ['멜로/로맨스']]]\n"
     ]
    }
   ],
   "source": [
    "print(positive_sessions[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "2a2bb72f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['02_2659_01',\n",
       " '03_3732_26',\n",
       " '03_1512_14',\n",
       " '01_0020_26',\n",
       " '01_0338_14',\n",
       " '01_2463_17',\n",
       " '02_2999_28',\n",
       " '01_0068_17',\n",
       " '02_0685_04',\n",
       " '02_2747_08']"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "positive_ids[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "bc46677c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of train = 90069 , val = 5003 , test = 5005\n"
     ]
    }
   ],
   "source": [
    "train_num = int(len(positive_sessions) * 0.9)\n",
    "dev_test_num = int(len(positive_sessions) * 0.05)\n",
    "train_sessions, dev_sessions, test_sessions = positive_sessions[:train_num], positive_sessions[train_num: train_num + dev_test_num], positive_sessions[train_num + dev_test_num:]\n",
    "print('number of train =', len(train_sessions), ', val =', len(dev_sessions), ', test =', len(test_sessions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "132fe371",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of word2vec training sentences = 1419213\n"
     ]
    }
   ],
   "source": [
    "train_texts = []\n",
    "for train_session in train_sessions:\n",
    "    train_texts += train_session[0]\n",
    "    train_texts.append(train_session[1])\n",
    "print('number of word2vec training sentences =', len(train_texts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "b7540667",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "\n",
    "# word2vec 학습\n",
    "model = Word2Vec(sentences = train_texts, vector_size = 200, window = 7, min_count = 5, workers = 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "52e2e01e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total num of words = 19451\n",
      "first word = \"C\"\n",
      "last word = \"재청\"\n"
     ]
    }
   ],
   "source": [
    "print('total num of words =', len(model.wv.key_to_index))\n",
    "print('first word = \"%s\"'%model.wv.index_to_key[0])\n",
    "print('last word = \"%s\"'%model.wv.index_to_key[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "4707535f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('친구', 0.6844412684440613), ('식구', 0.6206466555595398), ('지인', 0.6158685684204102), ('아이', 0.6108291745185852), ('동생', 0.6014763712882996), ('동료', 0.6002482771873474), ('자녀', 0.6002011299133301), ('백성', 0.594779372215271), ('부대원', 0.5899234414100647), ('사람', 0.5896921753883362)]\n"
     ]
    }
   ],
   "source": [
    "# word2vec이 잘 학습되었는지 여러가지 테스트를 수행하자.\n",
    "print(model.wv.most_similar(\"가족\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab6ad5d7",
   "metadata": {},
   "source": [
    "## 데이터 저장\n",
    "\n",
    "`embeddings.pkl`과 `vocab.txt`를 생성한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "b8453ed9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "\n",
    "# 예시: Word2Vec 모델을 불러왔을 경우\n",
    "#model = Word2Vec.load(f\"data/word2vec_{prefix}.model\")\n",
    "\n",
    "# 이제 .wv 사용 가능\n",
    "with open(f\"data/vocab_{prefix}.txt\", \"w\", encoding=\"utf-8\") as file:\n",
    "    for i, key in enumerate(model.wv.index_to_key):\n",
    "        file.write(f\"{key}\\t{i}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "67a3bda5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "# ✅ 첫 번째 벡터 사이즈를 기반으로 결정\n",
    "vector_size = model.vector_size\n",
    "vectors = [np.zeros(vector_size)]\n",
    "\n",
    "for word in model.wv.index_to_key:\n",
    "    vec = model.wv.get_vector(word)\n",
    "    if vec.shape[0] != vector_size:\n",
    "        print(f\"❗ {word} 벡터 차원 불일치: {vec.shape}\")\n",
    "        continue  # 벡터 차원이 안 맞으면 무시\n",
    "    vectors.append(vec)\n",
    "\n",
    "# 최종 배열 생성\n",
    "new_embeddings = np.stack(vectors)\n",
    "\n",
    "# 저장\n",
    "with open(f'data/embeddings_{prefix}.pkl', 'wb') as f:\n",
    "    pickle.dump(new_embeddings, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "f435a5b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(f\"data/word2vec_{prefix}.model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50f4ec99",
   "metadata": {},
   "source": [
    "# 학습 데이터셋 준비"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "066fc318",
   "metadata": {},
   "outputs": [],
   "source": [
    "EOS_ID = model.wv.key_to_index['[SEP]'] + 1 if '[SEP]' in model.wv else 1\n",
    "UNK_ID = model.wv.key_to_index['[UNK]'] + 1 if '[UNK]' in model.wv else 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "2a86226f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "vocab = {}\n",
    "\n",
    "with open(f\"data/vocab_{prefix}.txt\", \"r\", encoding=\"utf-8\") as fr:\n",
    "    for idx, line in enumerate(fr):\n",
    "        line = line.strip().split(\"\\t\")\n",
    "        vocab[line[0]] = idx + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "b0501f6f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "353"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sample id 출력 확인\n",
    "vocab['가족']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2611b33b",
   "metadata": {},
   "source": [
    "**positive data 준비**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "f165912a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'SF': 0, '공포(호러)': 1, '드라마': 2, '멜로/로맨스': 3, '미스터리': 4, '스릴러': 5, '액션': 6, '전쟁': 7, '코미디': 8, '판타지': 9}\n",
      "Total positive_data length: 100077\n"
     ]
    }
   ],
   "source": [
    "# 예시: 장르 사전 만들기\n",
    "genre_set = set()\n",
    "for unit in positive_sessions:\n",
    "    # unit[3]는 기존에 genre_info (리스트 형태)\n",
    "    for genre in unit[3]:\n",
    "        genre_set.add(genre)\n",
    "        \n",
    "genre_to_id = {genre: idx for idx, genre in enumerate(sorted(genre_set))}\n",
    "print(genre_to_id)\n",
    "\n",
    "# positive_data를 만들 때 genre 정보를 숫자 레이블로 변환하여 추가\n",
    "positive_data = []\n",
    "positive_str2 = []\n",
    "\n",
    "for unit, unit_str in zip(positive_sessions, positive_str):\n",
    "    narrative = unit[1]  # 내러티브 토큰 리스트\n",
    "    context = unit[0]    # 대화(utterance) 토큰 리스트\n",
    "    # 문자열로 되어있는 장르 정보를 숫자 리스트로 변환 (여러 개면 리스트 그대로)\n",
    "    genre_info = [genre_to_id[g] for g in unit[3]]\n",
    "    \n",
    "    narrative_id = [vocab.get(word, UNK_ID) for word in narrative]\n",
    "    context_id = [[vocab.get(word, UNK_ID) for word in sent] for sent in context]\n",
    "\n",
    "    if len(narrative_id) == 0 or len(context_id) == 0:\n",
    "        print('empty narrative found. skipping...')\n",
    "        print(unit_str)\n",
    "        continue\n",
    "\n",
    "    # 숫자 레이블로 변환한 genre_info도 데이터에 함께 추가\n",
    "    data = [context_id, narrative_id, 1, genre_info]\n",
    "    positive_data.append(data)\n",
    "    positive_str2.append(unit_str)\n",
    "\n",
    "print(\"Total positive_data length:\", len(positive_data))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "6bb0022a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[[1, 3, 12, 4, 2692, 1241, 38, 1, 3, 30, 8, 15, 793, 512, 16, 2], [1738, 1053, 5397, 6275, 1508, 8016, 8, 2883, 173, 115, 10, 8094, 150, 1153, 41, 15159, 2219, 37, 797, 14, 1124, 95, 102, 2], [1, 3, 50, 25, 1, 3, 56, 25, 1, 18, 169, 4, 793, 512, 5, 121, 14, 603, 16, 2], [227, 25, 1, 18, 1069, 35, 1, 18, 88, 796, 12, 4, 1, 3, 7, 38, 1, 18, 448, 10, 1708, 299, 10, 46, 2], [386, 127, 25, 128, 925, 22, 135, 21, 1161, 97, 185, 2, 786, 26, 24, 14, 402, 37, 964, 45, 34, 2], [1, 3, 7, 13, 1, 18, 88, 796, 12, 10, 555, 32, 5097, 5, 46, 2], [872, 108, 9, 1249, 4, 139, 229, 7253, 5546, 140, 3385, 28, 2], [122, 69, 21, 599, 2377, 255, 107, 126, 21], [1, 18, 1069, 4, 1, 18, 88, 796, 12, 8, 15, 4980, 8, 2674, 26, 428, 14, 885, 11, 2], [1, 18, 1069, 4, 2485, 7275, 5, 138, 14, 274, 27, 191, 2], [29]], [1, 3, 50, 25, 1, 3, 56, 25, 1, 18, 169, 4, 1, 3, 7, 5, 924, 152, 2609, 63, 1, 3, 7, 13, 1, 18, 88, 796, 12, 10, 4980, 16, 2], 1, [5]], [[[1, 3, 7, 13, 1, 3, 19, 38, 788, 8, 109, 2], [1, 3, 7, 6, 212, 2187, 16, 2], [108, 9, 58, 1193, 1446, 3312, 28, 2], [128, 656, 33, 119, 4, 122, 69, 24, 113, 42, 43, 21], [58, 9, 671, 67, 8, 82, 41, 110, 145, 113, 15, 2], [12827, 84, 54, 4, 260, 5, 6194, 49, 186, 42, 43, 21], [788, 638, 40, 13, 1, 3, 7, 6, 1749, 33, 1363, 1239, 16, 2], [1, 3, 19, 13, 1, 3, 7, 5, 775, 2856, 11, 2], [58, 9, 110, 964, 113, 15, 2], [58, 262, 31, 231, 1574, 322, 42, 205, 34, 21], [285, 8, 4, 225, 43, 116, 2], [855, 244, 114, 107, 28, 68, 253, 1096, 83, 598, 123, 207, 68], [1, 3, 7, 13, 67, 8, 82, 131, 14, 46, 2], [1, 3, 19, 13, 1, 3, 7, 5, 801, 4, 11, 2], [58, 9, 108, 1193, 1546, 157, 14, 5442, 15, 2], [1016, 26, 343, 2, 66, 400, 28, 398, 2], [1, 3, 7, 13, 1, 3, 19, 17, 24, 8, 10178, 947, 3772, 16, 2], [788, 638, 40, 13, 362, 5, 75, 32, 3535, 10, 664, 2], [392, 1081, 1, 3, 7, 13, 413, 193, 274, 27, 191, 2], [1, 3, 36, 13, 1, 3, 7, 8, 15, 622, 728, 319, 10, 863, 161, 2], [335, 42, 391, 54, 72, 2, 1076, 22, 59, 85, 655, 2], [66, 33, 119, 91, 1640, 34, 21, 1574, 97, 59, 438, 186, 42, 205, 34, 21], [2255, 2, 1, 3, 7, 258, 9, 9664, 642, 8, 14487, 5, 177, 1645, 34, 2], [997, 97, 59, 438, 48, 395, 21], [573, 1638, 2170, 34, 2, 66, 400, 72, 2], [1, 3, 7, 13, 1, 3, 36, 17, 24, 8, 1462, 553, 850, 16, 2], [66, 282, 1202, 61, 112, 8, 34, 2], [1, 3, 7, 13, 1, 3, 36, 8, 228, 302, 133, 28, 46, 2], [1, 3, 7, 13, 1, 3, 19, 8, 15, 289, 22, 59, 198, 881, 5, 723, 11, 2], [1, 3, 7, 13, 2316, 31, 3307, 362, 17, 2151, 6, 2495, 51, 5, 114, 15, 184, 2], [29]], [1, 3, 19, 13, 1, 3, 7, 5, 4290, 131, 14, 577, 63, 25, 1, 3, 36, 13, 1, 3, 7, 5, 377, 97, 131, 14, 46, 2], 1, [3, 2]], [[[1, 3, 7, 35, 1, 3, 19, 4, 571, 5, 933, 16, 2], [724, 1089, 5, 305, 871, 1, 3, 44, 35, 1, 3, 23, 10, 441, 2], [1, 3, 7, 4, 1315, 15, 1, 3, 23, 8, 15, 245, 10, 46, 2], [1, 3, 23, 4, 74, 40, 8, 15, 1, 3, 12, 9, 12065, 4065, 31, 142, 57, 49, 1404, 2], [215, 39, 75, 26, 1247, 49, 24, 16, 2], [388, 74, 9, 1164, 33, 528, 123, 40, 5, 3725, 2], [1, 3, 12, 4, 1, 3, 7, 8, 15, 3113, 3031, 26, 10, 1164, 16, 2], [1, 3, 7, 9, 3187, 230, 10688, 5, 141, 1107, 26, 1247, 129, 25, 1164, 5, 48, 55, 41, 74, 9, 325, 10, 198, 528, 10688, 5, 470, 14, 401, 509, 5, 3861, 107, 49, 1, 3, 12, 17, 24, 5, 1404, 2], [1712, 2016, 49, 24, 22, 32, 1, 3, 7, 8, 15, 4153, 5, 2614, 16, 2], [1, 3, 7, 4, 3035, 5, 442, 4, 11, 2], [29]], [1, 3, 7, 4, 3784, 243, 6, 2113, 273, 1361, 5, 141, 933, 16, 2, 388, 1, 3, 23, 37, 270, 1, 3, 12, 9, 142, 57, 111, 697, 5, 121, 4, 11, 2, 388, 1, 3, 7, 4, 74, 9, 73, 5, 198, 1164, 33, 1361, 123, 10, 121, 4, 11, 2], 1, [2, 8, 3]], [[[1, 3, 12, 13, 224, 6, 2377, 616, 2288, 14, 20, 11, 2], [12705, 11327, 8, 228, 3974, 16, 2], [439, 4, 12705, 11327, 9, 112, 57, 95, 102, 2], [1, 3, 7, 9, 2111, 4154, 8, 20, 4, 1093, 6, 294, 2], [117, 2573, 5, 5522, 45, 97, 1842, 2], [264, 8420, 207, 2], [1, 3, 7, 9, 3241, 223, 10, 1213, 14, 272, 6, 12263, 3425, 11, 2], [272, 9226, 5, 731, 177, 1, 3, 44, 17, 3303, 6, 237, 3735, 308, 2], [1, 3, 7, 17, 148, 6, 8458, 201, 11, 2], [1, 3, 12, 13, 1, 3, 7, 17, 148, 5, 249, 63, 179, 11, 2], [29]], [1, 3, 7, 9, 340, 27, 590, 378, 4, 11, 2], 1, [3, 6]], [[[1, 3, 30, 6, 1, 18, 88, 5, 3203, 27, 5742, 14, 20, 11, 2], [117, 4852, 665, 68, 241, 463, 68], [1, 3, 7, 9, 1, 3, 50, 9, 706, 871, 1179, 40, 2128, 5, 65, 2], [1, 3, 7, 9, 1179, 40, 17, 9052, 352, 90, 1964, 5, 3476, 22, 1120, 2263, 10357, 5, 116, 154, 62, 14, 46, 2], [1642, 127, 2, 525, 2094, 71, 34, 21, 387, 31, 212, 296, 42, 89, 52, 34, 2], [1607, 994, 8, 7202, 119, 2899, 5, 977, 1, 3, 12, 9, 1, 3, 7, 10, 1015, 2], [235, 1642, 127, 2404, 75, 4, 253, 6, 956, 41, 34, 21, 125, 39, 3907, 27, 2404, 75, 171, 34, 2, 7925, 2040, 22, 26, 64, 14, 1704, 27, 5572, 113, 395, 68], [1, 3, 7, 9, 280, 1074, 4, 3185, 123, 8, 236, 16, 2], [1, 3, 7, 9, 381, 85, 1269, 517, 32, 273, 5499, 191, 2], [1, 3, 12, 9, 828, 13, 2477, 90, 155, 39, 765, 2732, 1578, 16, 2], [29]], [1, 3, 12, 9, 2404, 5, 75, 131, 32, 1, 3, 7, 17, 369, 5, 1015, 2], 1, [3]]]\n"
     ]
    }
   ],
   "source": [
    "print(positive_data[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "a74d8151",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100077, 100077, 100077)"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(positive_str), len(positive_str2), len(positive_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "9944ec56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[1, 3, 12, 4, 2692, 1241, 38, 1, 3, 30, 8, 15, 793, 512, 16, 2]], [1738, 1053, 5397, 6275, 1508, 8016, 8, 2883, 173, 115, 10, 8094, 150, 1153, 41, 15159, 2219, 37, 797, 14, 1124, 95, 102, 2], [1, 3, 50, 25, 1, 3, 56, 25, 1, 18, 169, 4, 1, 3, 7, 5, 924, 152, 2609, 63, 1, 3, 7, 13, 1, 18, 88, 796, 12, 10, 4980, 16, 2], [1738, 1053, 5397, 6275, 1508, 8016, 8, 2883, 173, 115, 10, 8094, 150, 1153, 41, 15159, 2219, 37, 797, 14, 1124, 95, 102, 2], 1, [5]]\n",
      "[[[1, 3, 12, 4, 2692, 1241, 38, 1, 3, 30, 8, 15, 793, 512, 16, 2]], [58, 9, 1691, 546, 23, 17, 1278, 90, 155, 39, 58, 9, 1278, 90, 51, 5, 332, 39, 254, 729, 495, 276, 22, 11, 2, 203, 2425, 1118, 9201, 379, 126, 301, 2], [1, 3, 50, 25, 1, 3, 56, 25, 1, 18, 169, 4, 1, 3, 7, 5, 924, 152, 2609, 63, 1, 3, 7, 13, 1, 18, 88, 796, 12, 10, 4980, 16, 2], [1738, 1053, 5397, 6275, 1508, 8016, 8, 2883, 173, 115, 10, 8094, 150, 1153, 41, 15159, 2219, 37, 797, 14, 1124, 95, 102, 2], 0, [5]]\n",
      "[[[1072, 493, 4, 3049, 3551, 8, 1909, 35, 2023, 33, 3585, 5, 513, 1082, 2]], [1072, 493, 4, 563, 5, 1341, 32, 7198, 575, 6897, 8, 562, 10, 132, 202, 516, 449, 5, 986, 11, 2], [2448, 22, 4, 621, 47, 31, 76, 22, 4, 1, 3, 7, 13, 1649, 10, 121, 173, 2196, 31, 215, 251, 1354, 17, 769, 10, 132, 86, 111, 51, 5, 302, 179, 11, 2], [1072, 493, 4, 563, 5, 1341, 32, 7198, 575, 6897, 8, 562, 10, 132, 202, 516, 449, 5, 986, 11, 2], 1, [5, 1, 2]] [[[1072, 493, 4, 3049, 3551, 8, 1909, 35, 2023, 33, 3585, 5, 513, 1082, 2]], [1, 3, 7, 13, 4757, 10, 6611, 337, 291, 1755, 104, 2], [2448, 22, 4, 621, 47, 31, 76, 22, 4, 1, 3, 7, 13, 1649, 10, 121, 173, 2196, 31, 215, 251, 1354, 17, 769, 10, 132, 86, 111, 51, 5, 302, 179, 11, 2], [1072, 493, 4, 563, 5, 1341, 32, 7198, 575, 6897, 8, 562, 10, 132, 202, 516, 449, 5, 986, 11, 2], 0, [5, 1, 2]] [[[1072, 493, 4, 3049, 3551, 8, 1909, 35, 2023, 33, 3585, 5, 513, 1082, 2]], [1, 18, 137, 6, 881, 5, 249, 16, 2], [2448, 22, 4, 621, 47, 31, 76, 22, 4, 1, 3, 7, 13, 1649, 10, 121, 173, 2196, 31, 215, 251, 1354, 17, 769, 10, 132, 86, 111, 51, 5, 302, 179, 11, 2], [1072, 493, 4, 563, 5, 1341, 32, 7198, 575, 6897, 8, 562, 10, 132, 202, 516, 449, 5, 986, 11, 2], 0, [5, 1, 2]]\n",
      "[[[392, 1081, 1, 3, 7, 35, 1, 3, 12, 35, 1, 3, 23, 35, 1, 3, 19, 4, 5574, 7, 8, 109, 2]], [1, 3, 7, 35, 1, 3, 12, 35, 1, 3, 23, 35, 1, 3, 19, 4, 1983, 7, 3157, 3380, 5, 46, 2], [1, 3, 7, 35, 1, 3, 12, 35, 1, 3, 23, 35, 1, 3, 19, 4, 638, 40, 17, 9904, 33, 1292, 8, 236, 16, 2], [1, 3, 7, 35, 1, 3, 12, 35, 1, 3, 23, 35, 1, 3, 19, 4, 1983, 7, 3157, 3380, 5, 46, 2], 1, [2, 8]] [[[392, 1081, 1, 3, 7, 35, 1, 3, 12, 35, 1, 3, 23, 35, 1, 3, 19, 4, 5574, 7, 8, 109, 2]], [1, 3, 7, 9, 87, 8, 1245, 11, 2], [1, 3, 7, 35, 1, 3, 12, 35, 1, 3, 23, 35, 1, 3, 19, 4, 638, 40, 17, 9904, 33, 1292, 8, 236, 16, 2], [1, 3, 7, 35, 1, 3, 12, 35, 1, 3, 23, 35, 1, 3, 19, 4, 1983, 7, 3157, 3380, 5, 46, 2], 0, [2, 8]] [[[392, 1081, 1, 3, 7, 35, 1, 3, 12, 35, 1, 3, 23, 35, 1, 3, 19, 4, 5574, 7, 8, 109, 2]], [1, 3, 7, 6, 298, 41, 314, 1951, 40, 226, 2], [1, 3, 7, 35, 1, 3, 12, 35, 1, 3, 23, 35, 1, 3, 19, 4, 638, 40, 17, 9904, 33, 1292, 8, 236, 16, 2], [1, 3, 7, 35, 1, 3, 12, 35, 1, 3, 23, 35, 1, 3, 19, 4, 1983, 7, 3157, 3380, 5, 46, 2], 0, [2, 8]]\n",
      "688840 688840\n",
      "total train count = 2478150\n",
      "total val count = 686300\n",
      "total test count = 688840\n",
      "Total number of utterances in test: 68884\n"
     ]
    }
   ],
   "source": [
    "dev_test_num = int(len(positive_data) * 0.05)\n",
    "train, dev, test = positive_data[:train_num], positive_data[train_num: train_num + dev_test_num], positive_data[train_num + dev_test_num:]\n",
    "train_ids, dev_ids, test_ids = positive_ids[:train_num], positive_ids[train_num: train_num + dev_test_num], positive_ids[train_num + dev_test_num:]\n",
    "\n",
    "import random\n",
    "train_all, dev_all, test_all = [], [], []\n",
    "\n",
    "# 학습 샘플 생성 (genre_info 추가)\n",
    "for context_id, narrative_id, _, genre_info in train:\n",
    "    num_context = len(context_id)\n",
    "    for i in range(1, num_context):\n",
    "        context = context_id[:i]\n",
    "        response = context_id[i]\n",
    "        # 정답(positive) 샘플: 마지막 항목에 genre_info 추가\n",
    "        train_all.append([context, response, narrative_id, response, 1, genre_info])\n",
    "        flag = True\n",
    "        while flag:\n",
    "            random_idx = random.randint(0, len(positive_data) - 1)\n",
    "            random_context = positive_data[random_idx][0]\n",
    "            random_idx_2 = random.randint(0, len(random_context) - 1)\n",
    "            random_response = random_context[random_idx_2]\n",
    "            if len(response) != len(random_response):\n",
    "                flag = False\n",
    "                train_all.append([context, random_response, narrative_id, response, 0, genre_info])\n",
    "            else:\n",
    "                for idx, wid in enumerate(response):\n",
    "                    if wid != random_response[idx]:\n",
    "                        flag = False\n",
    "                        train_all.append([context, random_response, narrative_id, response, 0, genre_info])\n",
    "                        break\n",
    "\n",
    "print(train_all[0])\n",
    "print(train_all[1])\n",
    "\n",
    "# 개발(Validation) 샘플 생성 (genre_info 추가)\n",
    "dev_all_ids = []\n",
    "for i_dev, (context_id, narrative_id, _, genre_info) in enumerate(dev):\n",
    "    num_context = len(context_id)\n",
    "    for i in range(1, num_context):\n",
    "        context = context_id[:i]\n",
    "        response = context_id[i]\n",
    "        dev_all.append([context, response, narrative_id, response, 1, genre_info])\n",
    "        dev_all_ids.append(dev_ids[i_dev])\n",
    "        count = 0\n",
    "        negative_samples = []\n",
    "        while count < 8:\n",
    "            random_idx = random.randint(0, len(positive_data) - 1)\n",
    "            random_context = positive_data[random_idx][0]\n",
    "            random_idx_2 = random.randint(0, len(random_context) - 1)\n",
    "            random_response = random_context[random_idx_2]\n",
    "            if random_response not in negative_samples and random_response != [EOS_ID]:\n",
    "                if len(response) != len(random_response):\n",
    "                    dev_all.append([context, random_response, narrative_id, response, 0, genre_info])\n",
    "                    negative_samples.append(random_response)\n",
    "                    dev_all_ids.append(dev_ids[i_dev])\n",
    "                    count += 1\n",
    "                else:\n",
    "                    for idx, wid in enumerate(response):\n",
    "                        if wid != random_response[idx]:\n",
    "                            dev_all.append([context, random_response, narrative_id, response, 0, genre_info])\n",
    "                            negative_samples.append(random_response)\n",
    "                            dev_all_ids.append(dev_ids[i_dev])\n",
    "                            count += 1\n",
    "                            break\n",
    "        if response == [EOS_ID]:\n",
    "            dev_all.append([context, [EOS_ID], narrative_id, response, 1, genre_info])\n",
    "        else:\n",
    "            dev_all.append([context, [EOS_ID], narrative_id, response, 0, genre_info])\n",
    "        dev_all_ids.append(dev_ids[i_dev])\n",
    "\n",
    "print(dev_all[0], dev_all[1], dev_all[2])\n",
    "\n",
    "# 테스트 샘플 생성 (genre_info 추가)\n",
    "test_all = []\n",
    "test_all_ids = []\n",
    "test_num_context = []\n",
    "for i_test, (context_id, narrative_id, _, genre_info) in enumerate(test):\n",
    "    num_context = len(context_id)\n",
    "    test_num_context.append(num_context - 1)\n",
    "    for i in range(1, num_context):\n",
    "        context = context_id[:i]\n",
    "        response = context_id[i]\n",
    "        test_all.append([context, response, narrative_id, response, 1, genre_info])\n",
    "        test_all_ids.append(test_ids[i_test])\n",
    "        count = 0\n",
    "        negative_samples = []\n",
    "        while count < 8:\n",
    "            random_idx = random.randint(0, len(positive_data) - 1)\n",
    "            random_context = positive_data[random_idx][0]\n",
    "            random_idx_2 = random.randint(0, len(random_context) - 1)\n",
    "            random_response = random_context[random_idx_2]\n",
    "            if random_response not in negative_samples and random_response != [EOS_ID]:\n",
    "                if len(response) != len(random_response):\n",
    "                    test_all.append([context, random_response, narrative_id, response, 0, genre_info])\n",
    "                    negative_samples.append(random_response)\n",
    "                    test_all_ids.append(test_ids[i_test])\n",
    "                    count += 1\n",
    "                else:\n",
    "                    for idx, id in enumerate(response):\n",
    "                        if id != random_response[idx]:\n",
    "                            test_all.append([context, random_response, narrative_id, response, 0, genre_info])\n",
    "                            negative_samples.append(random_response)\n",
    "                            test_all_ids.append(test_ids[i_test])\n",
    "                            count += 1\n",
    "                            break\n",
    "        if response == [EOS_ID]:\n",
    "            test_all.append([context, [EOS_ID], narrative_id, response, 1, genre_info])\n",
    "        else:\n",
    "            test_all.append([context, [EOS_ID], narrative_id, response, 0, genre_info])\n",
    "        test_all_ids.append(test_ids[i_test])\n",
    "\n",
    "print(test_all[0], test_all[1], test_all[2])\n",
    "print(len(test_all_ids), len(test_all))\n",
    "print('total train count =', len(train_all))\n",
    "print('total val count =', len(dev_all))\n",
    "print('total test count =', len(test_all))\n",
    "print('Total number of utterances in test:', np.sum(np.array(test_num_context)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "47cd19f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_numpy_from_nonfixed_2d_array(aa, max_sentence_len=50, max_num_utterance=10, padding_value=0):\n",
    "    PAD_SEQUENCE = np.array([0] * max_sentence_len)\n",
    "    rows = np.empty([0, max_sentence_len], dtype='int')\n",
    "    aa = aa[-max_num_utterance:]\n",
    "    for a in aa:\n",
    "        sentence_len = len(a)\n",
    "        if sentence_len < max_sentence_len:\n",
    "            rows  = np.append(rows, [np.pad(a, (0, max_sentence_len-sentence_len), 'constant', constant_values=padding_value)[:max_sentence_len]], axis=0)\n",
    "        else:\n",
    "            rows = np.append(rows, [a[:max_sentence_len]], axis=0)\n",
    "    num_utterance = len(aa)\n",
    "    if num_utterance < max_num_utterance:\n",
    "        rows = np.append(rows, [PAD_SEQUENCE]*(max_num_utterance-num_utterance), axis=0)\n",
    "    # add empty +1 sentence\n",
    "    rows = np.append(rows, [PAD_SEQUENCE], axis=0)\n",
    "    #return np.concatenate(rows, axis=0).reshape(-1, max_sentence_len)\n",
    "    return rows\n",
    "\n",
    "def get_numpy_from_nonfixed_1d_array(a, max_sentence_len=50, padding_value=0):\n",
    "    sentence_len = len(a)\n",
    "    if sentence_len < max_sentence_len:\n",
    "        return np.pad(a, (0, max_sentence_len-sentence_len), 'constant', constant_values=padding_value)\n",
    "    else:\n",
    "        return np.array(a[:max_sentence_len])\n",
    "\n",
    "cc_test_data = [\n",
    "        [1,2],\n",
    "        [4,5,6],\n",
    "        [7]\n",
    "     ]\n",
    "#get_numpy_from_nonfixed_2d_array(cc_test_data, max_sentence_len=5, max_num_utterance=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "22242ce1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2478150/2478150 [04:50<00:00, 8531.37it/s]\n",
      "100%|██████████| 686300/686300 [01:18<00:00, 8734.60it/s]\n",
      "100%|██████████| 688840/688840 [01:17<00:00, 8869.16it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "def pad_process(data, max_sentence_len=50, max_num_utterance=10):\n",
    "    utterance = []\n",
    "    response = []\n",
    "    narrative = []\n",
    "    gt_response = []\n",
    "    y_true = []\n",
    "    genre_info_list = []  # 장르 정보는 그대로 리스트로 보관\n",
    "    \n",
    "    for unit in tqdm(data):\n",
    "        # unit[0]: context (list of utterance token ID lists)\n",
    "        utterance.append(get_numpy_from_nonfixed_2d_array(unit[0], max_sentence_len=max_sentence_len, max_num_utterance=max_num_utterance))\n",
    "        # unit[1]: response (list of token IDs)\n",
    "        response.append(get_numpy_from_nonfixed_1d_array(unit[1], max_sentence_len=max_sentence_len))\n",
    "        # unit[2]: narrative (list of token IDs)\n",
    "        narrative.append(get_numpy_from_nonfixed_1d_array(unit[2], max_sentence_len=max_sentence_len))\n",
    "        # unit[3]: gt_response (list of token IDs)\n",
    "        gt_response.append(get_numpy_from_nonfixed_1d_array(unit[3], max_sentence_len=max_sentence_len))\n",
    "        # unit[4]: y_true (정답 여부)\n",
    "        y_true.append(unit[4])\n",
    "        # unit[5]: genre_info (숫자로 매핑된 장르 리스트)\n",
    "        genre_info_list.append(unit[5])\n",
    "        \n",
    "    utterance = np.stack(utterance)\n",
    "    response = np.stack(response)\n",
    "    narrative = np.stack(narrative)\n",
    "    gt_response = np.stack(gt_response)\n",
    "    y_true = np.stack(y_true)\n",
    "    return (utterance, response, narrative, gt_response, y_true, genre_info_list)\n",
    "    \n",
    "# 패딩 적용 (각 데이터셋에 대하여)\n",
    "train_pad = pad_process(train_all)\n",
    "dev_pad = pad_process(dev_all)\n",
    "test_pad = pad_process(test_all)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "003946db",
   "metadata": {},
   "source": [
    "**학습데이터셋 저장**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "f4d8dcef",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f'data/train_{prefix}.pkl', 'wb') as f:\n",
    "    pickle.dump(train_pad, f)\n",
    "with open(f'data/dev_{prefix}.pkl', 'wb') as f:\n",
    "    pickle.dump(dev_pad, f)\n",
    "with open(f'data/test_{prefix}.pkl', 'wb') as f:\n",
    "    pickle.dump(test_pad, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "a3ac8f6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f'data/positive_{prefix}.pkl', \"wb\") as f:\n",
    "    pickle.dump(positive_data, f)\n",
    "with open(f'data/positive_str_{prefix}.pkl', \"wb\") as f:\n",
    "    pickle.dump(positive_str2, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "642fbe23",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f'data/test_all_ids_{prefix}.pkl', \"wb\") as f:\n",
    "    pickle.dump(test_all_ids, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "449089fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "for unit, unit_str in zip (positive_data, positive_str2):\n",
    "    len_unit = len(unit[0])\n",
    "    len_unit_str = len(unit_str['script'])\n",
    "    if len_unit != len_unit_str+1:\n",
    "        print(len_unit, len_unit_str)\n",
    "    #print(unit[0])\n",
    "    #print(unit_str['script'])\n",
    "    #break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "e6bbcbe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "genre_list = sorted(list(genre_set))\n",
    "genre_to_id = {genre: i for i, genre in enumerate(genre_list)}\n",
    "id_to_genre = {v: k for k, v in genre_to_id.items()}\n",
    "\n",
    "def get_dat(index, data_pad, ids=None):\n",
    "    # 패딩된 데이터에서 각 항목 추출 (마지막 요소는 장르 정보, 숫자 리스트)\n",
    "    utterances = data_pad[0][index]\n",
    "    response    = data_pad[1][index]\n",
    "    narrative   = data_pad[2][index]\n",
    "    gt_response = data_pad[3][index]\n",
    "    y_true      = data_pad[4][index]\n",
    "    genres      = data_pad[5][index]  # 예: [2, 5]\n",
    "\n",
    "    # 패딩(0) 제거\n",
    "    narrative = narrative[narrative != 0]\n",
    "    response  = response[response != 0]\n",
    "    gt_response = gt_response[gt_response != 0]\n",
    "\n",
    "    narrative_str = tokenizer.convert_tokens_to_string([model.wv.index_to_key[k - 1] for k in narrative])\n",
    "    response_str  = tokenizer.convert_tokens_to_string([model.wv.index_to_key[k - 1] for k in response])\n",
    "    gt_response_str = tokenizer.convert_tokens_to_string([model.wv.index_to_key[k - 1] for k in gt_response])\n",
    "\n",
    "    utterance_str = [''] * 10\n",
    "    for i in range(10):\n",
    "        utt = utterances[i]\n",
    "        utt = utt[utt != 0]\n",
    "        if len(utt) == 0:\n",
    "            break\n",
    "        utterance_str[i] = tokenizer.convert_tokens_to_string([model.wv.index_to_key[k - 1] for k in utt])\n",
    "    \n",
    "    id_ = ids[index] if ids is not None else None\n",
    "    \n",
    "    # 숫자형 장르 정보를 문자열로 변환\n",
    "    genre_names = [id_to_genre.get(g, \"UNK\") for g in genres]\n",
    "\n",
    "    return id_, narrative_str, response_str, gt_response_str, y_true, utterance_str, genre_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "ee8435ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def browse_dat(index, data_pad):\n",
    "    utterances = data_pad[0][index]\n",
    "    response    = data_pad[1][index]\n",
    "    narrative   = data_pad[2][index]\n",
    "    gt_response = data_pad[3][index]\n",
    "    y_true      = data_pad[4][index]\n",
    "    genres      = data_pad[5][index]\n",
    "\n",
    "    narrative = narrative[narrative != 0]\n",
    "    response  = response[response != 0]\n",
    "    gt_response = gt_response[gt_response != 0]\n",
    "\n",
    "    print('N:', tokenizer.convert_tokens_to_string([model.wv.index_to_key[k - 1] for k in narrative]))\n",
    "    print('R:', tokenizer.convert_tokens_to_string([model.wv.index_to_key[k - 1] for k in response]))\n",
    "    print('T:', tokenizer.convert_tokens_to_string([model.wv.index_to_key[k - 1] for k in gt_response]))\n",
    "    print('Y_true:', y_true)\n",
    "    \n",
    "    # 숫자형 장르 정보를 문자열로 변환하여 출력\n",
    "    genre_names = [id_to_genre.get(g, \"UNK\") for g in genres]\n",
    "    print('Genre:', genre_names)\n",
    "    \n",
    "    for i in range(10):\n",
    "        utt = utterances[i]\n",
    "        utt = utt[utt != 0]\n",
    "        if len(utt) == 0:\n",
    "            break\n",
    "        print('U:', tokenizer.convert_tokens_to_string([model.wv.index_to_key[k - 1] for k in utt]))\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "32158ead",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N: 고아인 C008은 냄새에 민감한 아이로 자라나. 팔려 간다.\n",
      "R: 신생아를 낳은 여인은 도망치다가 살인자로 몰렸고 사형을 당한다.\n",
      "T: 신생아를 낳은 여인은 도망치다가 살인자로 몰렸고 사형을 당한다.\n",
      "Y_true: 1\n",
      "Genre: ['드라마', '스릴러']\n",
      "U: 생선을 팔던 한 여인은 아이를 낳았다.\n",
      "U: 갓난아기는 바닥에 버려진 채 온 힘을 다해서 울음을 터뜨린다.\n",
      "\n",
      "N: 고아인 C008은 냄새에 민감한 아이로 자라나. 팔려 간다.\n",
      "R: 마찬가지죠, C001 경사님도 제 뒷조사 열심히 하시던데요?\n",
      "T: 신생아를 낳은 여인은 도망치다가 살인자로 몰렸고 사형을 당한다.\n",
      "Y_true: 0\n",
      "Genre: ['드라마', '스릴러']\n",
      "U: 생선을 팔던 한 여인은 아이를 낳았다.\n",
      "U: 갓난아기는 바닥에 버려진 채 온 힘을 다해서 울음을 터뜨린다.\n",
      "\n",
      "N: 고아인 C008은 냄새에 민감한 아이로 자라나. 팔려 간다.\n",
      "R: C001이 호텔 밖으로 이동한다.\n",
      "T: 신생아를 낳은 여인은 도망치다가 살인자로 몰렸고 사형을 당한다.\n",
      "Y_true: 0\n",
      "Genre: ['드라마', '스릴러']\n",
      "U: 생선을 팔던 한 여인은 아이를 낳았다.\n",
      "U: 갓난아기는 바닥에 버려진 채 온 힘을 다해서 울음을 터뜨린다.\n",
      "\n",
      "N: 고아인 C008은 냄새에 민감한 아이로 자라나. 팔려 간다.\n",
      "R: 동생은 식당 밖으로 나와 울먹이는 C002를 달래준다.\n",
      "T: 신생아를 낳은 여인은 도망치다가 살인자로 몰렸고 사형을 당한다.\n",
      "Y_true: 0\n",
      "Genre: ['드라마', '스릴러']\n",
      "U: 생선을 팔던 한 여인은 아이를 낳았다.\n",
      "U: 갓난아기는 바닥에 버려진 채 온 힘을 다해서 울음을 터뜨린다.\n",
      "\n",
      "N: 고아인 C008은 냄새에 민감한 아이로 자라나. 팔려 간다.\n",
      "R: C002는 먹을 수 있는 것과 그렇지 않은 것을 골라낸다.\n",
      "T: 신생아를 낳은 여인은 도망치다가 살인자로 몰렸고 사형을 당한다.\n",
      "Y_true: 0\n",
      "Genre: ['드라마', '스릴러']\n",
      "U: 생선을 팔던 한 여인은 아이를 낳았다.\n",
      "U: 갓난아기는 바닥에 버려진 채 온 힘을 다해서 울음을 터뜨린다.\n",
      "\n",
      "N: 고아인 C008은 냄새에 민감한 아이로 자라나. 팔려 간다.\n",
      "R: C011는 무시한다.\n",
      "T: 신생아를 낳은 여인은 도망치다가 살인자로 몰렸고 사형을 당한다.\n",
      "Y_true: 0\n",
      "Genre: ['드라마', '스릴러']\n",
      "U: 생선을 팔던 한 여인은 아이를 낳았다.\n",
      "U: 갓난아기는 바닥에 버려진 채 온 힘을 다해서 울음을 터뜨린다.\n",
      "\n",
      "N: 고아인 C008은 냄새에 민감한 아이로 자라나. 팔려 간다.\n",
      "R: 왜 이러세요. 부장님.\n",
      "T: 신생아를 낳은 여인은 도망치다가 살인자로 몰렸고 사형을 당한다.\n",
      "Y_true: 0\n",
      "Genre: ['드라마', '스릴러']\n",
      "U: 생선을 팔던 한 여인은 아이를 낳았다.\n",
      "U: 갓난아기는 바닥에 버려진 채 온 힘을 다해서 울음을 터뜨린다.\n",
      "\n",
      "N: 고아인 C008은 냄새에 민감한 아이로 자라나. 팔려 간다.\n",
      "R: 당신이 호텔에 묶여있는 동안 계속 곁에 있기로 다짐했어요.\n",
      "T: 신생아를 낳은 여인은 도망치다가 살인자로 몰렸고 사형을 당한다.\n",
      "Y_true: 0\n",
      "Genre: ['드라마', '스릴러']\n",
      "U: 생선을 팔던 한 여인은 아이를 낳았다.\n",
      "U: 갓난아기는 바닥에 버려진 채 온 힘을 다해서 울음을 터뜨린다.\n",
      "\n",
      "N: 고아인 C008은 냄새에 민감한 아이로 자라나. 팔려 간다.\n",
      "R: 무게를 이기지 못한 조각상이 용암 아래로 떨어진다.\n",
      "T: 신생아를 낳은 여인은 도망치다가 살인자로 몰렸고 사형을 당한다.\n",
      "Y_true: 0\n",
      "Genre: ['드라마', '스릴러']\n",
      "U: 생선을 팔던 한 여인은 아이를 낳았다.\n",
      "U: 갓난아기는 바닥에 버려진 채 온 힘을 다해서 울음을 터뜨린다.\n",
      "\n",
      "N: 고아인 C008은 냄새에 민감한 아이로 자라나. 팔려 간다.\n",
      "R: [SEP]\n",
      "T: 신생아를 낳은 여인은 도망치다가 살인자로 몰렸고 사형을 당한다.\n",
      "Y_true: 0\n",
      "Genre: ['드라마', '스릴러']\n",
      "U: 생선을 팔던 한 여인은 아이를 낳았다.\n",
      "U: 갓난아기는 바닥에 버려진 채 온 힘을 다해서 울음을 터뜨린다.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(110,120): \n",
    "    browse_dat(i, test_pad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "82471e24",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "0b4d3606",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['id', 'Narrative', 'Response', 'GT_Response', 'y_true', 'score', 'R2@1', 'R10@1', 'R10@2', 'R10@5', 'MRR', 'AVG', 'genre', 'U01', 'U02', 'U03', 'U04', 'U05', 'U06', 'U07', 'U08', 'U09', 'U10']\n"
     ]
    }
   ],
   "source": [
    "column_names = ['id', 'Narrative', 'Response', 'GT_Response', 'y_true', 'score', 'R2@1', 'R10@1', 'R10@2', 'R10@5', 'MRR', 'AVG','genre']\n",
    "for i in range(10):\n",
    "    column_names.append('U%02d'%(i+1))\n",
    "print(column_names)\n",
    "df = pd.DataFrame(columns=column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "e54583f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 688840/688840 [02:00<00:00, 5703.76it/s]\n"
     ]
    }
   ],
   "source": [
    "n = len(test_all_ids)\n",
    "data_dict_all = []\n",
    "for i in tqdm(range(n)):\n",
    "    id_, narrative_str, response_str, gt_response_str, y_true, utterance_str, genre = get_dat(i, test_pad, test_all_ids)\n",
    "    data_dict = { }\n",
    "    data_dict['id'] = id_\n",
    "    data_dict['Narrative'] = narrative_str\n",
    "    data_dict['Response'] = response_str\n",
    "    data_dict['GT_Response'] = gt_response_str\n",
    "    data_dict['y_true'] = y_true\n",
    "    data_dict['genre'] = genre\n",
    "    for i in range(10):\n",
    "        data_dict[f'U%02d'%(i+1)] = utterance_str[i]\n",
    "    #new_row = pd.Series(data_dict)\n",
    "    #df = pd.concat([df, new_row.to_frame().T], ignore_index=True)\n",
    "    data_dict_all.append(data_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "f01f0132",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame.from_dict(data_dict_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "cd97d2ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_excel(f'test_output_{prefix}.xlsx', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "54a11abe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['id', 'Narrative', 'Response', 'GT_Response', 'y_true', 'genre', 'U01', 'U02', 'U03', 'U04', 'U05', 'U06', 'U07', 'U08', 'U09', 'U10']\n"
     ]
    }
   ],
   "source": [
    "column_names = ['id', 'Narrative', 'Response', 'GT_Response', 'y_true', 'genre']\n",
    "for i in range(10):\n",
    "    column_names.append(f'U%02d' % (i + 1))\n",
    "\n",
    "print(column_names)\n",
    "df = pd.DataFrame(columns=column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "44b58053",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 686300/686300 [02:02<00:00, 5598.34it/s]\n"
     ]
    }
   ],
   "source": [
    "n = len(dev_all_ids)\n",
    "data_dict_all = []\n",
    "for i in tqdm(range(n)):\n",
    "    id_, narrative_str, response_str, gt_response_str, y_true, utterance_str, genre = get_dat(i, test_pad, test_all_ids)\n",
    "    \n",
    "    data_dict = {\n",
    "        'id': id_,\n",
    "        'Narrative': narrative_str,\n",
    "        'Response': response_str,\n",
    "        'GT_Response': gt_response_str,\n",
    "        'y_true': y_true,\n",
    "        'genre': genre  # genre 추가\n",
    "    }\n",
    "    for j in range(10):\n",
    "        data_dict[f'U%02d' % (j + 1)] = utterance_str[j]\n",
    "\n",
    "    data_dict_all.append(data_dict)\n",
    "\n",
    "df = pd.DataFrame.from_dict(data_dict_all)\n",
    "df.to_excel(f'dev_output_{prefix}.xlsx', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbe32838",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c38ee808",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Jupyter_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
